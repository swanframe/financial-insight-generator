# Configuration for Financial Insight Generator

data:
  # Path to the input CSV/Excel file with raw transactions
  input_path: "data/raw/sample_transactions.csv"

  # Optional: specify the date format if your dates are in a known format.
  # If omitted, the loader will let pandas infer the format.
  date_format: "%Y-%m-%d"
  parse_dates: true

columns:
  # Column mapping: logical field -> column name in your CSV

  # Required fields
  date: "order_date"
  amount: "total_price"

  # Optional fields (set to null if not available)
  cost: "cost"
  category: "category"
  product: "product_name"
  customer_id: "customer_id"
  channel: "sales_channel"

analytics:
  # Granularity for time-based metrics: "day", "week", or "month"
  time_granularity: "month"

  # "Top N" size for categories/products/customers
  top_n: 5

  # Simple anomaly detection settings
  anomaly_lookback_days: 30
  anomaly_sigma_threshold: 2.0

output:
  # Control what gets saved to disk
  save_clean_data: true
  clean_data_path: "data/processed/cleaned_transactions.csv"

  save_metrics: true
  metrics_path: "data/processed/metrics.json"

  save_report: true
  report_path: "reports/financial_insights.txt"

ui:
  # Language for user-facing output (CLI, reports, etc.)
  # Supported values: "en" (English), "id" (Bahasa Indonesia)
  language: "id"

llm:
  # Global toggle for LLM features. When false, FIG behaves as a pure, template-based tool.
  enabled: true

  # Logical provider name; used by fig.llm_client to decide which SDK / HTTP client to call.
  # Examples: "openai", "gemini", "deepseek", "custom".
  provider: "openai"

  # Default model name for the chosen provider.
  model: "gpt-4.1-mini"

  # Sampling / generation parameters.
  temperature: 0.3
  max_tokens: 800

  # Name of the environment variable that holds your LLM API key.
  # Example: "OPENAI_API_KEY"
  api_key_env_var: "OPENAI_API_KEY"

  # Network safety limits (seconds).
  timeout_seconds: 30

  # Report/assistant behavior mode:
  # - "template" -> use only the built-in template-based generator (no LLM calls)
  # - "llm"      -> use only the LLM-based generator
  # - "hybrid"   -> use the template output as context for the LLM
  mode: "llm"

  # Safety limit for how much structured context we send into prompts
  # (metrics bundle, samples of the cleaned DataFrame, etc.).
  max_context_chars: 12000

embeddings:
  # Embeddings configuration is provider-agnostic and can be shared by
  # vector stores, RAG pipelines, etc.
  #
  # If this section is omitted, FIG derives sensible defaults from the llm
  # configuration (same provider and API key env var).
  provider: "dummy"
  model: "ignored-for-dummy"
  api_key_env_var: "IGNORED_FOR_DUMMY"
  timeout_seconds: 30

vector_store:
  # Master toggle for vector features. When false, FIG ignores any vector indexing
  # or search commands and behaves exactly like a non-RAG tool.
  enabled: true

  # Vector DB backend. Later you can add other providers like "qdrant", "pinecone", etc.
  provider: "chroma"          # "chroma" (persistent) or "in_memory"

  # Where to store the Chroma database on disk (local-first).
  persist_path: "data/vector_store"
  collection_name: "fig_transactions"

  # Default number of similar items to retrieve when no explicit top_k is given.
  default_top_k: 5

orchestration:
  # High-level orchestration / engine selection for LLM/RAG flows.
  # - "native": use FIG's built-in Python pipeline.
  # - "langchain": use LangChain-based chains defined in src/fig/langchain_*.py.
  engine: "langchain"